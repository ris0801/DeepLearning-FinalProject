# DeepLearning FinalProject

## Aim
In order to comprehend the distinguishing factors between neural networks that exhibit strong generalization capabilities and those that do not.

## Experiments performed: 
1. The impact of explicit regularization techniques such as data augmentation, weight decay, and dropout on deep learning models.
2. The influence of implicit regularization methods, specifically BatchNorm, on the performance and generalization of neural networks.
3. Analyzing the effects of input data corruption, including pixel shuffle, Gaussian pixels, and random pixels, on the training and convergence of deep learning models.
4. Investigating the impact of label corruption at varying levels, ranging from 1% to 100%, on the training process and generalization ability of neural networks.

## Models used:
AlexNet, Inception_v3, Wide ResNet, Inception (tiny), MLP-512.

## Results: 
All the obtained results are attached in Results section in the prepared report

##Team Members:

Rishabh Singh(rbs7261)
Sarthak Chowdhary (sc9865)
Samir Ahmed Talkal (st4703)

## Instructions to run the python files

All the files have been ran on NYU HPC cluster, and graphs have been built using Google colab. 
ALl the files can independently produce results, and can be seprately run in colab, each file contains appropraite code for reproducing all graphs provided in Results in Report. 

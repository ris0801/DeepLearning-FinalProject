# DeepLearning FinalProject

##Experiments performed: 
1. The impact of explicit regularization techniques such as data augmentation, weight decay, and dropout on deep learning models.
2. The influence of implicit regularization methods, specifically BatchNorm, on the performance and generalization of neural networks.
3. Analyzing the effects of input data corruption, including pixel shuffle, Gaussian pixels, and random pixels, on the training and convergence of deep learning models.
4. Investigating the impact of label corruption at varying levels, ranging from 1% to 100%, on the training process and generalization ability of neural networks.

##Models used:
AlexNet, Inception_v3, Wide ResNet, Inception (tiny), MLP-512.
